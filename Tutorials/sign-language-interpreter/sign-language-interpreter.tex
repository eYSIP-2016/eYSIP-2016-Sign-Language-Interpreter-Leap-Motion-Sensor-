%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	e-Yantra, IIT-Bombay

%	Document Author: Sanket R. Bhimani
%	Date: 16-August,2012
%	Last Editted by: Saurav
%   Date Last updated: 31-05-2016 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[11pt,a4paper]{article}
\newcommand\tab[1][1cm]{\hspace*{#1}}
\usepackage{graphicx}
\title{\textbf{\Huge{Sign Language Interpreter}}\vspace{6mm}\\Using Leap Motion Sensor}
\usepackage[utf8]{inputenc}
 \usepackage{graphicx}
\usepackage{listings}
\usepackage{color}
 \usepackage{hyperref}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}
 
\author{Sanket R Bhimani}
\date{\today}

\begin{document}
	\maketitle
	\newpage
	\tableofcontents
	\newpage
	\section{Tutorial Name:}
	\begin{center}\huge{Sign language Interpreter}\\\Large{Using Leap Motion Sensor}\end{center}
	\vspace{15mm}
	\textbf{\Large{1.1 Objective:}}\\
	\vspace{1mm}\\
	Objective behind this project is to help hearing-impaired and verbally challenged people to communicate with normal people. So this system will try to interpret the sign language into natural voice.\\
	\vspace{6mm}\\
	\textbf{\Large{1.2 Abstract:}}\\
	\vspace{1mm}\\
	Here we are using leap motion sensor for detecting gesture and then we will convert in audio with mp3 module. Before conversion we are converting a set of words to a natural sentence using NLTK and ginger grammar API.
	\newpage
	\section{Prerequisites:}
	\vspace{1cm}
	\tab Before going ahead it is better to learn these basic things
    \vspace{1cm}
	\begin{itemize}
	    \item Basic knowledge of Python
	    \item Basic knowledge of Linux
	    \item Basic knowledge of NLTK
	    \item Basic Electronics knowledge
	\end{itemize}
	\newpage
	\section{Requirement}
	\vspace{1cm}
	\begin{itemize}
	    \item Hardware Requirements:
	    \begin{itemize}
	        \item Galileo Board
	        \item Leap Motion Sensor
	        \item microSD card of minimum size of 8 GB with card reader
	        \item speaker
	        \item MP3 Module
	        \item LAN cable or wifi module(for connecting board with PC)
	    \end{itemize}
	    \vspace{1cm}
	    \item Software Requirements:
	    \begin{itemize}
	        \item Python editor
	        \item Leap sdk
	        \item NLTK for Python
	        \item Tornado Web-socket
	        \item Bitwise SSH Client
	        \item Wireshark(optional)
	    \end{itemize}
	\end{itemize}
	\newpage
	\section{Theory and Description}
	\vspace{1cm} \tab{Essential topics to learn before starting the project:}
	\begin{itemize}
	    \item \begin{itemize}
	        \item Leap motion library for Python
	        \item LeapTrainer.js
	        \item Python programming
	        \item Websocket
	        \item getting started with Galileo board
	        \item interfacing with MP3 module
	    \end{itemize}
	\end{itemize}
	\vspace{1cm}
	\textbf{\huge{4.1 Leap motion Library:}}
	\vspace{1cm}\\
	\tab{In python library all class names are given as human body part so it is very easy to understand. Main and useful classes are,}
	\begin{itemize}
	    \item Frame
        \item Hand
        \item Finger
        \item Pointable
        \item Arm
	\end{itemize}
	
	\tab{For getting data from leap sensor first we need to register listener class with controller object and for that first we need to create listener class. And register that listener class object with controller object you can find the basic code for getting basic frame data from given code examples “connection\_with\_leap.py”}\\
	\newpage
	\textbf{\Large{4.1.1 Frame:}}
	\vspace{0.5cm}\\
    \tab{A frame is an object which can be generated from controller object. Leap sensor sends data through this frame object in listener class. From frame object, we can find all other objects. Means all row data comes with the frame. As per leap FPS on\_frame(self,controller) method of listener class is called with a new frame. Each frame has its unique id and time stamp.}\\

    \tab{In on\_frame method from controller object, we can get the current frame. It also supports serialization.}\\

    \vspace{3cm}
    \textbf{\Large{4.1.2 Hand:}}
	\vspace{0.5cm}\\
	\tab{Hands are the main entity tracked by the Leap Motion controller. The Hand class represents a physical hand detected by the Leap. A Hand object provides access to lists of its pointables as well as attributes describing the hand position, orientation, and movement.}\\
	\tab{From current frame object, we can get the current set of detected hand as an array. We can get hand from hands object by these properties,}\\
	\begin{lstlisting}[language=Python, caption=getting frame and hand object]
frame = controller.frame() 
hands = frame.hands

leftmost = hands.leftmost
rightmost = hands.rightmost
frontmost = hands.frontmost\end{lstlisting}
\vspace{0.5cm}
    We can get these characteristics from hand object,
    \begin{itemize}
        \item isRight, isLeft — Boolean variable which checks whether the hand detected by leap motion sensor is right or left.
        \item palm\_position — The center of the palm measured in millimeters from the Leap Motion origin.
        \item palm\_velocity — The speed and movement direction of the palm in millimeters per second.
        \item palm\_normal — A vector perpendicular to the plane formed by the palm of the hand. The vector points downward out of the palm.
        \item direction — A vector pointing from the center of the palm toward the fingers.
        \item grab\_strength, pinch\_strength — Describe the posture of the hand.
        \item stabilized\_palm\_position — The stabilized palm position of this Hand.
        \item Motion factors — Provide relative scale, rotation, and translation factors for movement between two frames.
    \end{itemize}
    \vspace{.5cm}
    We can get a pitch, yaw, and roll of hand by these properties,
    \vspace{.5cm}
    \begin{lstlisting}[language=Python, caption=getting pitch\, yaw and roll from hand object]
pitch = hand.direction.pitch
yaw = hand.direction.yaw
roll = hand.palm_normal.roll\end{lstlisting}
\vspace{2cm}
    \textbf{\Large{4.1.3 Figures \& Pointables:}}
	\vspace{0.5cm}\\
	\tab{Fingers are Pointable objects that the Leap Motion software has classified as a finger. Get valid Finger objects from a Frame or a Hand object.}\\
	\vspace{.5cm}
	This can be generated from frame object or hand object. As this way,
	\begin{lstlisting}[caption=getting figures and pointables]
frame_pointables = frame.pointables
hand_pointables = hand.pointables
known_pointable = hand.pointable(known_id)
finger = Finger(pointable)\end{lstlisting}
\vspace{1cm}
\tab{If pointable is detected as the particular figure then and then only it can be returned as finger object. But mostly this pointer is detected as a finger.}
\vspace{.5cm}
We can get a bone object from finger object and get the position of all joints.
	\begin{lstlisting}
bone = finger.bone(Bone.TYPE_PROXIMAL)\end{lstlisting}
\vspace{2cm}
\textbf{inputs:}
\begin{itemize}
    \item 0 = TYPE\_METACARPAL – The metacarpal bone.
    \item 1 = TYPE\_PROXIMAL – The proximal phalanx; the closest bone segment to the hand.
    \item 2 = TYPE\_INTERMEDIATE – The intermediate, or middle, phalanx.
    \item 3 = TYPE\_DISTAL – The distal phalanx; the bone segment furthest from the hand.
\end{itemize}
\vspace{1cm}

\begin{lstlisting}

Joint_position = finger.joint_position(0)
    \end{lstlisting}
\vspace{1cm}

\textbf{inputs:}
\begin{itemize}
    \item 0 = JOINT\_MCP – The metacarpophalangeal joint, or knuckle, of the finger.
    \item 1 = JOINT\_PIP – The proximal interphalangeal joint of the finger. This joint is the middle joint of a finger.
    \item 2 = JOINT\_DIP – The distal interphalangeal joint of the finger. This joint is closest to the tip.
    \item 3 = JOINT\_TIP – The tip of the finger.
\end{itemize}
\vspace{1cm}
We can get this type of properties from pointable object,
\begin{itemize}
    \item id  -  A unique ID assigned to this Pointable object.
    \item Frame - The Frame associated with this Pointable object.
    \item Hand - The Hand associated with this finger.
    \item tip\_position - The tip position in millimeters from the Leap Motion origin.
    \item tip\_velocity - The rate of change of the tip position in millimeters/second.
    \item direction - The direction in which this finger is pointing.
    \item width - The estimated width of the finger in millimeters.
    \item length - The estimated length of the finger in millimeters.
    \item is\_finger - Whether or not the Pointable is classified as a finger.
    \item stabilized\_tip\_position - The stabilized tip position of this Pointable.
    \item time\_visible - The duration of time this Pointable has been visible to the Leap Motion Controller.
\end{itemize}
\vspace{2cm}
    \textbf{\Large{4.1.4 Arm:}}
	\vspace{0.5cm}\\
	\tab{There is also an arm object which gives data related elbow\_position, arm.wrist\_position, width, direction, the basis of all three axises.}\\
	\vspace{.5cm}
	Arm object is generated from hand object.
	\begin{lstlisting}
hand = frame.hands.frontmost
arm = hand.arm
\end{lstlisting}
\vspace{1cm}

\textbf{attributes:}
\begin{itemize}
    \item basis:\\
\tab{Basis vectors specify the orientation of an arm.}
    \begin{itemize}
        \item x\_basis. Perpendicular to the longitudinal axis of the arm; exits leterally through the sides of the wrist.
        \item y\_basis or up vector. Perpendicular to the longitudinal axis of the arm; exits the top and bottom of the arm. More positive in the upward direction.
        \item z\_basis. Aligned with the longitudinal axis of the arm. More positive toward the elbow.
    \end{itemize}
    \vspace{2cm}
    	\begin{lstlisting}
basis = arm.basis
x_basis = basis.x_basis
y_basis = basis.y_basis
z_basis = basis.z_basis
center=arm.elbow_position+(arm.wrist_position-arm.elbow_position*.05
arm_transform = Leap.Matrix(x_basis, y_basis, z_basis, center)
\end{lstlisting}
\vspace{1cm}
    \item direction - The normalized direction of the arm from elbow to wrist.
    \item elbow\_position - The position of the elbow.
    \item width - The average width of the arm.
    \item wrist\_position - The position of the wrist.
\end{itemize}
\vspace{3cm}
Here we will use only some properties like,
\begin{itemize}
    \item In hand,
    \begin{itemize}
        \item direction
        \item palm\_position
        \item stabilized\_palm\_position
    \end{itemize}
    \item In pointable,
    \begin{itemize}
        \item direction
        \item tip\_position
        \item stabilized\_tip\_position
    \end{itemize}
\end{itemize}
	\newpage
	\textbf{\huge{4.2 LeapTrainer.js:}}
	\vspace{1cm}\\
	\tab{This API is used gesture and pose learning and recognition framework for leap motion sensor. This is javascript based API, so using that framework we can detect gestures and poses in a web browser, and then using websocket it will go to python program.}
	\vspace{.5cm}
	So here is the overview of this API,
	\vspace{.5cm}
	\textbf{Uses:}
	\begin{lstlisting}
<script src="http://js.leapmotion.com/0.2.0/leap.min.js"></script>
<script src="/path/to/leaptrainer.min.js"></script>
\end{lstlisting}
\vspace{1cm}
\tab{You need to include these two files for using this framework. The first file is for connection with leap sensor and second for this framework.}
\tab{Then we need to create leap sensor’s object and trainer object}
\vspace{.5cm}
\begin{lstlisting}
var leapController = new Leap.Controller();
var trainer = new LeapTrainer.Controller({controller: leapController});
leapController.connect();
\end{lstlisting}
\vspace{1cm}
\textbf{4.2.1 Training the system:}\\
\vspace{.5cm}\\
\tab{We can train the system by adding new gesture or pose to system, and we can add new gesture or pose by calling this method,}
\begin{lstlisting}
trainer.create('Halt');
\end{lstlisting}
\vspace{.5cm}
\tab{here in place of ‘Halt’ give any name related to your gesture or pose. And we can do something when this gesture is recognized like,}
\begin{lstlisting}
trainer.on('Halt', function() { console.log('Stop right there!'); });
\end{lstlisting}
\newpage

\begin{figure}
  \includegraphics[width=\linewidth]{13.png}
  \caption{LeapTrainer-ui}
  \includegraphics[width=\linewidth]{14.png}
  \caption{LeapTrainer-ui}
\end{figure}


\textbf{4.2.2 Importing and Exporting from LeapTrainer:}
\vspace{.5cm}\\
\tab{This feature is most useful in real application. Because normally wat happened whatever data we create like learning gestures or poses, are temporary means once you reload the script all objects will be reinitialized so all saved gestures or poses data will be lost. So by using this feature we can save that data.}\\
\tab{This framework exports data in the form of JSON and also accepts imports in the form of JSON.}
\begin{lstlisting}
var savedGesture = trainer.toJSON('Halt'); 
//Exporting to JSON

trainer.fromJSON(savedData);
//Importing from JSON
\end{lstlisting}
This can be used for saving and fetching gesture or pose data into or from file\\
\vspace{2cm}\\
\tab{More details about LeapTrainer library is can be found on github. But we are not using this library because there is many error. like, it autometically detects gsutures, so when we are going to perform any gestures it will recognize another one before performance. so it become annoying. So, I have decided to make my own gesture rcognizing system we will see about it letter}
\newpage
\textbf{\huge{4.3 Installation of Linux image in Galileo board:}}
	\vspace{1cm}\\
	\tab{In Galileo board, we can install Linux, windows IOT or os x. So the interface with board become easy. Here we will use Linux image. Because currently windows IOT image is no more supported. And everyone is not used to OS X. For that, we need to write Linux image on the sd card. From sd card, Galileo board will boot.}\\
	\vspace{.5cm}\\
	\textbf{\Large{4.3.1 How to write Linux image on Galileo}}\\
	\vspace{.3cm}\\
	\textbf{Step 1:}\\
	Download Linux image from this location.\\
	https://downloadmirror.intel.com/26028/eng/iot-devkit-prof-dev-image-galileo-20160525.zip\\
	You will get one zip file extract it. You will find one ‘.direct’. You need to write this into sd card\\
	\vspace{.3cm}\\
	\textbf{Step 2:}\\
	Download this software from this location,\\
http://download.softpedia.com/dl/d5a643aad00816ee735372c2d530a1a2/57584b56/100173006/software/cd\_dvd\_tools/Win32DiskImager-0.9.5-binary.zip\\
this will be helpful for writing an image in sd card.\\
\vspace{.3cm}\\
\begin{figure}
  \includegraphics[width=\linewidth]{1.png}
  \caption{Win32 disk Imager window}
\end{figure}
\vspace{.3cm}\\
Select Image File which you have downloaded earlier in ‘Image File’ tab(‘.direct’ file)\\
Select sd card in ‘Device tab’\\
Now click on ‘Write’ button, it will take some time. And wait until write process completes\\
\vspace{.3cm}\\
	\textbf{Step 3:}\\
	\vspace{.3cm}\\
	\tab{Now insert sd card in galileo board and connect LAN cable with your PC and galileo board. Now connect the power supply with galileo board.}

\tab{now the board will automatically boot.}
\tab{There will be blinking LED near sd card port. Wait until it stops lighting. Because it shows IO related sd card. So, when it become stop, IO related sd card is also stopped so the booting process is completed.}
\tab{Now you need to find  the IP address of your board. For that, you can use any network packet tracer software like Wireshark, ettercap or any other.}

\tab{I am using Wireshark.}

\tab{Now open Wireshark select ethernet as a network adapter. It will start tracing all packet data on ethernet. But here on an ethernet network, there is only two devices are there your PC and board. So there is only two IP address data. (ignore IPs with 255, 251, 224 and more than 200 value in last two part like 192.168.0.255 or 255.255.255.0 ) so find your PC IP address using this command }
\begin{lstlisting}
ipconfig (for windows)}
ifconfig (for Linux)}
\end{lstlisting}
\tab{Now another IP showing on Wireshark is your board IP. }

\begin{figure}
  \includegraphics[width=\linewidth]{2.png}
  \caption{Your IP address}
\end{figure}

Your Wireshark windows:\\
\begin{figure}
  \includegraphics[width=\linewidth]{3.png}
  \caption{wireshark window}
\end{figure}
\newpage
	\textbf{Step 3:}\\
	\vspace{.3cm}\\
	Now, from any SSH client make connection with board and use. :?)\\
(I’m using Bitvise SSH client)\\
Enter IP address and username: root there is no password.\\
\begin{figure}
  \includegraphics[width=\linewidth]{4.png}
  \caption{SSH client}
\end{figure}
Click on login, and wait. Now two windows will open one for terminal and another for file transfer.
Your board is ready! :?)
\begin{figure}
  \includegraphics[width=9cm]{5.png}
  \caption{SFTP window}
    \includegraphics[width=\linewidth]{6.png}
  \caption{SSH terminal}
\end{figure}

	\newpage
     
	\textbf{\huge{4.4 Interfacing MP3 module:}}\\
    \vspace{.3cm}\\
	\tab{MP3 module is used to play audio files stored in separate memory card attached with module. Control of this module is done the hex code sent from any controller via UART port. Here we will send code from Galileo board from Rx Tx pin to the board, there are some specified method to generating hex code which you can find on it's data sheet. We will see about that in next topic}
 \begin{figure}
  \includegraphics[width=\linewidth]{15.png}
  \caption{Connection with Galileo board and speakers}
  \end{figure}
   \begin{figure}
  \includegraphics[width=\linewidth]{10.png}
  \caption{Pin diagram}
    \includegraphics[width=\linewidth]{11.png}
  \caption{Table for pin diagram}
\end{figure}

\newpage
	\section{Implementation:}
	\vspace{1cm}
	\begin{itemize}
	    \item Alphabet training and recognizing with leap motion
	    \item Make natural sentence from words using NLTK
	    \item Make simple player for MP3 module
	    \item Other Experiment for gesture recognizing.
	    \item First version of this system with LeapTrainer.js
	    \item second and final version of this system
	\end{itemize}
	\vspace{2cm}
	\textbf{\huge{5.1 Alphabet training and recognizing with leap motion:}}
	\vspace{1cm}\\
	\tab{Here we want to make a programme which records or say learns pose hand and then it will recognize. Means there will be two mode learning and recognizing. In Learning mode we need to perform pose so this pose will be saved for future reference and need to provide one alphabet for that pose. and In recognizing mode we need to perform specific pose so programme will recognize that pose and give related alphabet}\\
	\tab{Here we have used pointables x direction, pointable z direction, pointable x stabilized tip position, pointable z stabilized tip position, stabilized palm position of x axis and z axis.}\\
	\tab{Here we want to implement a programme which is independent of hand's position means we can perform action at any position in range of sensor.  so we have take data relative palm center so we have used stabilized palm position but here only x and z axis is useful because we don't need to care about height from sensor.}\\
	\tab{Here we will find radius distance from palm center to every tip of  finger  so we need to find mean position of tip's x and z co-ordinates related to palm center}\\
	\tab{We are also taking direction vectored so we can increase range of detection of radius distance}\\
	\tab{Then sum interface related programming. whole programme is in python language you can find at github \url{https://github.com/sanketbhimani/eYSIP-2016-Sign-Language-Interpreter-Leap-Motion-Sensor-/blob/master/alphabet_using_leap.py}}\\
	\vspace{.5cm}\\
	\textbf{Problems faced:}\\
	\tab{While copping object I was directly write like a = b. so programme was not working properly and only large performed pose goes to every object. I was not understanding what was happening. what was happening, when I write a = b it just transfer a pointer no new memory is allocated so when I change value for one object all values will change because they all are pointing to same memory location. then i used a = copy.deepcopy(b) then it will allocate new memory for that object. and it is working fine!!!}
	\newpage
	\vspace{2cm}
	\textbf{\huge{5.2 Make natural sentence from words using NLTK:}}
	\vspace{1cm}\\
	\tab{In sign language, normally whole sentence is not actioned mean little grammar thing like "is" are not much important. because action just want to show the understanding about what he want to tell. they not try to build a complete sentence with correct grammar.}\\
    \tab{So, this programme will help to create a proper natural sentence with correct grammar. like if you do input like "WHAT NAME YOU", means you want to ask a name someone this should be like "WHAT IS YOU NAME?" so this type of conversation is done by this programme}\\
    \tab{For that I have use NLTK library, NLTK have lot of data to compare and detect what is this word means this is verb, pronoun, noun etc. Let take one example, "make a note" here note is noun but "note it down" here note is verb so for different cases same word can perform different role, so NLTK helps you to detect the role of words, it also helps you to organize sentence and word at smart way.}\\
    \tab{After detecting the role of this words, it's our job to make a proper sentence. This programme will put them in proper sequence and add necessary words like "is". then it will correct this sentence grammar. like if the input is like "WHAT NAME YOU" our programme generate new sentence like "WHAT IS YOU NAME" then after correcting grammar this will be like "WHAT IS YOUR NAME?" For correcting grammar I have use online API, it's ginger grammar. So you need to just send raw sentence and as a res ponce you will get proper correct sentence with some extra information as a JSON data you need to fetch correct sentence.}
    
    \tab{So you need to learn basic NLTK library and sending and receiving HTTP request and response using python.}
 \begin{figure}
  \includegraphics[width=\linewidth]{12.png}
  \caption{Screen-Shot for NLTK}
\end{figure}
    \newpage
	\textbf{\huge{5.3 Make simple player for MP3 module:}}
	\vspace{1cm}\\
	\tab{This is a simple programme designed to test and get learn an interfacing with MP3 module. This programme is basically mp3 player which have option of next, previous, play/pause, volume up and volume down.}\\
	\tab{This programme is in command line interface. And it generates appropriate hex code for perform certain task and send it to mp3 module through UART (Rx and Tx pin available on board). This includes of generating oh check bits and all.}\\
    Sample Code for this program is can find on github repository
	\newpage
	\textbf{\huge{5.4 Other Experiment for Gesture recognizing:}}
	\vspace{1cm}\\
	\tab{Now, we need to do gesture recognizing. means in first alphabet recognizing programme we have just recognize just pose but now we need to recognize whole motion. For that i was searching for works done on this topic. From leap developer community I found two useful repository first one is \$1 unistock gesture recognizing library. If know about \$1 and \$n algorithm of university of Sweden. This algorithm compares two pattern and gives percentages matches. So what i found that some one have made like using only one pointable we need to perform action and system will recognize that action. but here we can not use standard sign language because here only liner actions can be performed. And it with too much errors. Means it does mistakes so much while recognizing.}\\
	\tab{So, finally i decided to not to use this system. So I search another library named LeapTrainer.js. It was good and i start working on it.}\\
	\newpage

	\textbf{\huge{5.5 First version of this system with LeapTrainer.js:}}
	\vspace{1cm}\\
	\tab{This is based on web  pages meas whole system was web based. it runs as website. So gesture training and recognising happens on browser. Means, here we need to train the system first (Loading of gesture data), Then system will recognize that gestures. But problem was that after loading data when we reload the page or reopen the page  saved data ware destroyed. meas the data are not being permanently stored so we need to make some system for permanent saving of data.}\\
	\tab{As explined in theory there is a listener function for each events. So, when gesture-created event generated i store data of gestures in form of JSON using toJSON method. and in starting (means when we open the page) I load all gestures from JSON file. But here we want to make our system fully portable so JSON file is on Galileo board so the transfer of data is through Websocket. On board there is one python programme running which will create websocket server and our webpage be a websocket client. And all website data is also on board. we are just opening site from board to our laptop's browser. Here we need to connect Leap Motion Sensor on laptop side.}\\
	\vspace{1cm}\\
	\includegraphics[width=\linewidth]{7.png}
	\newpage
	\tab{But here we also use that NLTK programme to convert sentence in natural language with correct grammar so we need to first pass the output of this web application to this programme. Now, this programme requires more processing power so it can not be run on Galileo board so we need to run it on laptop so we will pass the output to python script running on y laptop and then it sent to Galileo board. here also communication will be done through websocket.}\\
	\vspace{1cm}\\
	\includegraphics[width=\linewidth]{8.png}
	\tab{There is also some problem like they made mistake many time to recognising gesture and its interface is also quite annoying. So we need to think something new.}
	\newpage
	
	\textbf{\huge{5.6 second version of this}}\\\textbf{\huge{system:}}
	\vspace{1cm}\\
	\tab{Now, I have tried everything available on Internet related gesture recognising but nothing was efficient. So, I have decided to make my own programme to recognising gestures. And worked! So I've create my own algorithm to recognising gestures. It's very similar to the programme of alphabet recognition.If you remember in that programme we are taking average of 300 frames and we are using that average value for future comparison so what was happening that small movement  while recording frames are gone by taking average  but now in this case we want to capture the movements so we can not average. But we can directly compare that much frame to that much frame. So if sufficient frame will match, then we can say that, this is that gesture.}\\
	\tab{whole programme interface is command-line. There will be two mode of programme, Train and Recognize. So, in train mode we need to perform gesture so system will remember as a future reference. And in recognize mode we need to perform this gesture and system will recognize that gesture and give word matched with it.}\\
	\tab{Here, in each mode there is a timer of 3 second so you can set a proper position and check whether your each hand is detected properly or not. So, capturing of frame will start after 3 second.
	For changing of options and all thing see programme on github. There is well commented code so you can change every option like change the timer second or say change no of frame captures.}\\
	\tab{Here, NLTK programme also merged with main programme so output is directly sent to board and from board to MP3 module, Here all gesture's data will be in laptop. And the communication with board and laptop will be through websocket.}\\
	\tab{Here whole system is in python. there is two programme one for server and one for client.}\\
	\newpage
	
	\begin{itemize}
        \item Task performed by client side programme:
        	\begin{itemize}
        	    \item Recognizing and Training of gesture.
        	    \item Storing and Loading data to and from JSON file stored on  same laptop/pc.
        	    \item Add necessary words and Correcting the position of words.
        	    \item Correcting the grammar.
        	    \item Sending sentence to board through websocket.
    	    \end{itemize}
        \item Task performed by server side programme:
            \begin{itemize}
                \item Receiving sentence sent from client side programme.
                \item Davide it into each word and find file name for that word in MP3 module
                \item And play appropriate file stored in mp3 module through sending proper hex code.
            \end{itemize}
	\end{itemize}
	\vspace{1cm}
	\includegraphics[width=\linewidth]{9.png}
	\tab{Now, there is only one problem need to be solved is speed. yes this system is very slow, because this algorithm  is making 300*300*2*5*2*(number of words loaded) comparisons so, it takes time. one solution for this is to decrease frame captured but it affects on efficiency and accuracy of program so I need to find another solution. }
	\newpage
	\textbf{\huge{5.7 Improved version of system}}\\
	\vspace{1cm}\\
	\tab{To make it faster I've tried many thing like with multi-threading, parallel processing, CUDA or many thing!!! but nothing works or say implemented! like in threading no improvement done because threading do same thing but with more threading so it not use more processing power. it just divide the process and do it in part.}\\
	\tab{And I tried to implement parallel processing but somehow I couldn't control on main thread so my main program is kept restarting and everything is happening so fast so i couldn't even found what is the error. So now I moved to CUDA. But it here also i failed to control on programme.}\\
	\tab{Now, i thought that let's do some improvement at normal way! So, I change the representation of data. first i was storing data in 4D list. So, it is becoming slow while changing the pointer from one location to another. So, I changed the representation of data storage to matrix form. So, it become just 2D list. And also I have converted it to numpy array. And also, all comparison I am doing is from numpy library. and it is the most efficient library to do process with matrix. So, finally it works! And now it is running faster. So there is no change in system but little change in calculation that makes my algorithm faster that is now I'm using.}

	
	\newpage
	
	
	\section{Exercise}
	\textbf{\huge{6.1 Finding Error rate for Alphabet\\recognising programme}}
	\vspace{1cm}\\
	Here is the result of experiments:
	\begin{center}
    \begin{itemize}
        \item "Yes" means recognize
        \item "No" means not recognize
\end{itemize}
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c}
    \hline
    	Alphabet&&test 1&test 2&test 3&test 4&test 5&test 6&test 7&test 8\\
         A& &yes&yes&yes&yes&yes&yes&yes&yes\\ 
         B& &yes&yes&yes&yes&yes&yes&yes&yes\\
         C& &yes&no&no&yes&yes&yes&yes&yes\\
         D& &no&yes&yes&yes&yes&yes&yes&yes\\
         E& &yes&yes&yes&yes&yes&yes&yes&yes\\
         F& &no&yes&yes&yes&no&yes&yes&yes\\
         G& &no&no&yes&yes&yes&yes&yes&yes\\
         H& &yes&yes&yes&yes&yes&yes&yes&yes\\
         I& &yes&yes&yes&yes&yes&yes&yes&yes\\
         J& &no&yes&yes&yes&yes&yes&yes&no\\
         K& &yes&yes&yes&yes&no&yes&yes&yes\\
         L& &yes&yes&yes&yes&yes&yes&yes&yes\\
         M& &yes&yes&yes&yes&yes&yes&yes&yes\\
         N& &no&yes&yes&no&no&yes&yes&yes\\
         O& &yes&yes&no&yes&yes&yes&yes&yes\\
         P& &yes&no&yes&yes&yes&yes&yes&yes\\
         Q& &yes&yes&yes&yes&yes&yes&no&yes\\
         R& &no&yes&no&yes&no&no&no&yes\\
         S& &yes&yes&yes&yes&yes&yes&yes&yes\\
         T& &yes&yes&yes&yes&yes&yes&yes&yes\\
         U& &yes&no&yes&yes&yes&yes&yes&yes\\
         V& &yes&yes&yes&yes&yes&yes&yes&yes\\
         W& &yes&yes&yes&yes&yes&yes&yes&yes\\
         X& &yes&yes&yes&yes&yes&yes&yes&yes\\
         Z& &no&yes&yes&yes&yes&yes&yes&yes\\
         
    \hline
    \end{tabular}
    \vspace{.1cm}
    \textbf {Result of alphabet recognition system}
    \end{center}
    \vspace{.3cm}
	total: 26*8 =  208\\
	Incorrect recognition: 23\\
	So rate is: 23/208 = 0.11057    :)
    \newpage
	\section{References}
    \begin{thebibliography}{li}
        \bibitem For learning python www.tutorialspoint.com/python/
        \bibitem For Various doubt solution www.stackoverflow.com/questions/tagged/python/
        \bibitem For understanding leap library in python www.developer.leapmotion.com/documentation/python/index.html
        \bibitem For javascript gesture recognizing and learning system  www.github.com/roboleary/LeapTrainer.js
        \bibitem For \$1 algorithm with Leap motion sensor www.github.com/liangzan/leap-demo
        \bibitem  www.depts.washington.edu/aimgroup/proj/dollar
        \bibitem  www.developer.leapmotion.com/libraries
        \bibitem  www.stackoverflow.com/questions/tagged/javascript
        \bibitem  www.stackoverflow.com/questions/tagged/jquery
        \bibitem  www.t4t5.github.io/sweetalert
        \bibitem  www.github.com/t4t5/sweetalert
        \bibitem  www.pypi.python.org/pypi/nltk
        \bibitem  www.nltk.org
        \bibitem  www.youtube.com/watch?v=FLZvOKSCkxY
        \bibitem  www.gingersoftware.com/grammarcheck
        \bibitem  www.github.com/zoncoen/python-ginger
        \bibitem  www.blog.livedoor.jp/xaicron/archives/54466736.html
        \bibitem  www.youtube.com/watch?v=gN3CGhFPF1s
        \bibitem  www.github.com/tornadoweb/tornado
        \bibitem  www.pypi.python.org/pypi/tornado
        \bibitem  www.gist.github.com/billroy/3761495
        \bibitem  www.stackoverflow.com/questions/2835559/parsing-values-from-a-json-file-in-python/
        \bibitem  www.stackoverflow.com/questions/29430333/simultaneously-reading-and-writing-to-json-file-in-python
        \bibitem  www.downloadmirror.intel.com/26028/eng/iot-devkit-prof-dev-image-galileo-20160525.zip
        \bibitem  www.software.intel.com/en-us/iot/hardware/galileo/downloads
        \bibitem  www.downloadmirror.intel.com/25384/eng/w\_galileo\_2015.0.010.exe
        \bibitem  www.youtube.com/watch?v=yrRMomesBKM
        \bibitem  www.github.com/intel-iot-devkit/mraa
        \bibitem  www.wiki.python.org/moin/ParallelProcessing
        \bibitem  www.stackoverflow.com/questions/20548628/how-to-do-parallel-programming-in-python
        \bibitem  http://spartanideas.msu.edu/2014/06/20/an-introduction-to-parallel-programming-using-pythons-multiprocessing-module/
        \bibitem  https://philipwfowler.github.io/2015-01-13-oxford/intermediate/python/04-multiprocessing.html
        \bibitem  http://materials.jeremybejarano.com/MPIwithPython/
        \bibitem  http://scipy.github.io/old-wiki/pages/ParallelProgramming
        \bibitem  https://wiki.python.org/moin/ParallelProcessing
        \bibitem  http://stackoverflow.com/questions/20548628/how-to-do-parallel-programming-in-python
        \bibitem  https://developer.nvidia.com/how-to-cuda-python
        \bibitem  https://mathema.tician.de/software/pycuda/
        \bibitem  http://stackoverflow.com/questions/5957554/python-gpu-programming
        \bibitem  http://stackoverflow.com/questions/20548628/how-to-do-parallel-programming-in-python
        \bibitem  http://www.numpy.org/
        \bibitem  https://docs.scipy.org/doc/numpy-dev/user/quickstart.html
        \bibitem  https://pypi.python.org/pypi/numpy
        \bibitem  http://docs.scipy.org/doc/numpy/reference/
\end{thebibliography}
\end{document}



